{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b161dd2-cf39-4393-8ce2-2bc1eb206792",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8908821417744983890d09b881c6f8b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from tensorflow.keras.layers import Concatenate\n",
    "import os\n",
    "import string\n",
    "from pickle import dump\n",
    "from pickle import load\n",
    "from keras.applications.xception import Xception #to get pre-trained model Xception\n",
    "from keras.applications.xception import preprocess_input\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer #for text tokenization\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Model, load_model\n",
    "from keras.layers import Input, Dense#Keras to build our CNN and LSTM\n",
    "from keras.layers import LSTM, Embedding, Dropout\n",
    "from tqdm.notebook import tqdm #to check loop progress\n",
    "tqdm().pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "442a5b6d-f04c-489e-9a4b-8ef2ccd9add8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of descriptions = 8092\n",
      "Length of vocabulary = 8422\n"
     ]
    }
   ],
   "source": [
    "# Load the document file into memory\n",
    "def load_fp(filename):\n",
    "    # Open file to read\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "# Get all images with their captions\n",
    "def img_capt(filename):\n",
    "    file = load_fp(filename)\n",
    "    captions = file.split('\\n')\n",
    "    descriptions = {}\n",
    "    for caption in captions[:-1]:\n",
    "        img, caption = caption.split('\\t')\n",
    "        if img[:-2] not in descriptions:\n",
    "            descriptions[img[:-2]] = [caption]\n",
    "        else:\n",
    "            descriptions[img[:-2]].append(caption)\n",
    "    return descriptions\n",
    "\n",
    "# Data cleaning function will convert all upper case alphabets to lowercase, remove punctuations, and words containing numbers\n",
    "import string\n",
    "\n",
    "def txt_clean(captions):\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for img, caps in captions.items():\n",
    "        for i, img_caption in enumerate(caps):\n",
    "            img_caption = img_caption.replace(\"-\", \" \")\n",
    "            descp = img_caption.split()\n",
    "            # Uppercase to lowercase\n",
    "            descp = [wrd.lower() for wrd in descp]\n",
    "            # Remove punctuation from each token\n",
    "            descp = [wrd.translate(table) for wrd in descp]\n",
    "            # Remove hanging 's and single letters\n",
    "            descp = [wrd for wrd in descp if len(wrd) > 1]\n",
    "            # Remove words containing numbers\n",
    "            descp = [wrd for wrd in descp if wrd.isalpha()]\n",
    "            # Convert back to string\n",
    "            img_caption = ' '.join(descp)\n",
    "            captions[img][i] = img_caption\n",
    "    return captions\n",
    "\n",
    "# Function to build vocabulary of all unique words\n",
    "def txt_vocab(descriptions):\n",
    "    vocab = set()\n",
    "    for key in descriptions.keys():\n",
    "        [vocab.update(d.split()) for d in descriptions[key]]\n",
    "    return vocab\n",
    "\n",
    "# To save all descriptions in one file\n",
    "def save_descriptions(descriptions, filename):\n",
    "    lines = []\n",
    "    for key, desc_list in descriptions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(key + '\\t' + desc)\n",
    "    data = \"\\n\".join(lines)\n",
    "    with open(filename, \"w\") as file:\n",
    "        file.write(data)\n",
    "\n",
    "# Set these paths according to your project folder\n",
    "dataset_text = r\"C:\\Users\\Asus\\OneDrive\\Documents\\Machine Learning\\Model\\Flickr8k_text\"\n",
    "dataset_images = r\"C:\\Users\\Asus\\OneDrive\\Documents\\Machine Learning\\Model\\Flicker8k_Dataset\"\n",
    "\n",
    "# Prepare text data\n",
    "filename = dataset_text + \"/\" + \"Flickr8k.token.txt\"\n",
    "\n",
    "# Load the file that contains all data and map them into a descriptions dictionary\n",
    "descriptions = img_capt(filename)\n",
    "print(\"Length of descriptions =\", len(descriptions))\n",
    "\n",
    "# Cleaning the descriptions\n",
    "clean_descriptions = txt_clean(descriptions)\n",
    "\n",
    "# Build vocabulary\n",
    "vocabulary = txt_vocab(clean_descriptions)\n",
    "print(\"Length of vocabulary =\", len(vocabulary))\n",
    "\n",
    "# Save all descriptions in one file\n",
    "save_descriptions(clean_descriptions, \"descriptions.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e26e565a-f660-4c49-aceb-c018e635baec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Xception( include_top=False, pooling='avg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ead90ec-913e-48b1-8434-f1f53cd1cd8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# def extract_features(directory):\n",
    "#     # Load the pre-trained Xception model, excluding the top layer\n",
    "#     model = Xception(include_top=False, pooling='avg')\n",
    "#     features = {}\n",
    "    \n",
    "#     # Loop over images in the directory\n",
    "#     for pic in tqdm(os.listdir(directory)):\n",
    "#         file_path = os.path.join(directory, pic)\n",
    "#         image = Image.open(file_path).convert('RGB')\n",
    "#         image = image.resize((299, 299))\n",
    "#         image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "#         # Preprocess image by scaling it\n",
    "#         image = image / 127.5 - 1.0\n",
    "#         feature = model.predict(image)\n",
    "        \n",
    "#         # Use pic as the key for features\n",
    "#         features[pic] = feature\n",
    "    \n",
    "#     return features\n",
    "\n",
    "# # Extract features and save them to a pickle file\n",
    "# dataset_images =  r\"C:\\Users\\Asus\\OneDrive\\Documents\\Machine Learning\\Model\\Flicker8k_Dataset\"\n",
    "# features = extract_features(dataset_images)\n",
    "# dump(features, open(\"features.p\", \"wb\"))\n",
    "\n",
    "# To load the features from the pickle file\n",
    "features = load(open(\"features.p\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01df35ef-7e18-48f1-8361-ba24afc6d47f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_doc(filename):\n",
    "    # Open the file and read the content\n",
    "    with open(filename, 'r') as file:\n",
    "        text = file.read()\n",
    "    return text\n",
    "\n",
    "def load_photos(filename):\n",
    "    file = load_doc(filename)\n",
    "    photos = file.split(\"n\")[:-1]\n",
    "    return photos\n",
    "\n",
    "def load_clean_descriptions(filename, photos):\n",
    "    # loading clean_descriptions\n",
    "    file = load_doc(filename)\n",
    "    descriptions = {}\n",
    "    for line in file.split(\"n\"):\n",
    "        words = line.split()\n",
    "        if len(words) < 1:\n",
    "            continue\n",
    "        image, image_caption = words[0], words[1:]\n",
    "        if image in photos:\n",
    "            if image not in descriptions:\n",
    "                descriptions[image] = []\n",
    "            desc = ' ' + \" \".join(image_caption) + ' '\n",
    "            descriptions[image].append(desc)\n",
    "    return descriptions\n",
    "\n",
    "def load_features(photos):\n",
    "    # loading all features\n",
    "    all_features = load(open(\"features.p\", \"rb\"))\n",
    "    # selecting only needed features\n",
    "    features = {k: all_features[k] for k in photos}\n",
    "    return features\n",
    "\n",
    "filename = dataset_text + \"/\" + \"Flickr_8k.trainImages.txt\"\n",
    "train_imgs = load_photos(filename)\n",
    "train_descriptions = load_clean_descriptions(\"descriptions.txt\", train_imgs)\n",
    "train_features = load_features(train_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86d2c4f9-89df-46ca-9324-b036fae5c898",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#convert dictionary to clear list of descriptions\n",
    "def dict_to_list(descriptions):\n",
    "    all_desc = []\n",
    "    for key in descriptions.keys():\n",
    "        [all_desc.append(d) for d in descriptions[key]]\n",
    "    return all_desc\n",
    "#creating tokenizer class\n",
    "#this will vectorise text corpus\n",
    "#each integer will represent token in dictionary\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "def create_tokenizer(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(desc_list)\n",
    "    return tokenizer\n",
    "# give each word an index, and store that into tokenizer.p pickle file\n",
    "tokenizer = create_tokenizer(train_descriptions)\n",
    "dump(tokenizer, open('tokenizer.p', 'wb'))\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "vocab_size #The size of our vocabulary is 7577 words.\n",
    "#calculate maximum length of descriptions to decide the model structure parameters.\n",
    "def max_length(descriptions):\n",
    "    desc_list = dict_to_list(descriptions)\n",
    "    return max(len(d.split()) for d in desc_list)\n",
    "max_length = max_length(descriptions)\n",
    "max_length #Max_length of description is 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24c56bba-20eb-48fd-9278-873afb52d32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data generator, used by model.fit_generator()\n",
    "def data_generator(descriptions, features, tokenizer, max_length):\n",
    "    while 1:\n",
    "        for key, description_list in descriptions.items():\n",
    "            # retrieve photo features\n",
    "            feature = features[key][0]\n",
    "            inp_image, inp_seq, op_word = create_sequences(tokenizer, max_length, description_list, feature)\n",
    "            yield [[inp_image, inp_seq], op_word]\n",
    "\n",
    "def create_sequences(tokenizer, max_length, desc_list, feature):\n",
    "    x_1, x_2, y = list(), list(), list()\n",
    "    # move through each description for the image\n",
    "    for desc in desc_list:\n",
    "        # encode the sequence\n",
    "        seq = tokenizer.texts_to_sequences([desc])[0]\n",
    "        # divide one sequence into various X, y pairs\n",
    "        for i in range(1, len(seq)):\n",
    "            # divide into input and output pair\n",
    "            in_seq, out_seq = seq[:i], seq[i]\n",
    "            # pad input sequence\n",
    "            in_seq = pad_sequences([in_seq], maxlen=max_length)[0]\n",
    "            # encode output sequence\n",
    "            out_seq = to_categorical([out_seq], num_classes=vocab_size)[0]\n",
    "            # store\n",
    "            x_1.append(feature)\n",
    "            x_2.append(in_seq)\n",
    "            y.append(out_seq)\n",
    "    \n",
    "    return np.array(x_1), np.array(x_2), np.array(y)\n",
    "\n",
    "# To check the shape of the input and output for your model\n",
    "[a, b], c = next(data_generator(train_descriptions, features, tokenizer, max_length))\n",
    "a.shape, b.shape, c.shape\n",
    "# ((47, 2048), (47, 32), (47, 7577))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46dda881-1e6c-4c4f-aa0c-67ad004bf936",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "  # features from the CNN model compressed from 2048 to 256 nodes\n",
    "   inputs1 = Input(shape=(2048,))\n",
    "   fe1 = Dropout(0.5)(inputs1)\n",
    "   fe2 = Dense(256, activation='relu')(fe1)\n",
    "  # LSTM sequence model\n",
    "   inputs2 = Input(shape=(max_length,))\n",
    "   se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "   se2 = Dropout(0.5)(se1)\n",
    "   se3 = LSTM(256)(se2)\n",
    "  # Merging both models\n",
    "   decoder1 = add([fe2, se3])\n",
    "   decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "   outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "  # merge it [image, seq] [word]\n",
    "   model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "   model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "  # summarize model\n",
    "   print(model.summary())\n",
    "   plot_model(model, to_file='model.png', show_shapes=True)\n",
    "return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b158606-a972-4596-a3da-a7fe0104c5ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train our model\n",
    "print('Dataset: ', len(train_imgs))\n",
    "print('Descriptions: train=', len(train_descriptions))\n",
    "print('Photos: train=', len(train_features))\n",
    "print('Vocabulary Size:', vocab_size)\n",
    "print('Description Length: ', max_length)\n",
    "model = define_model(vocab_size, max_length)\n",
    "epochs = 10\n",
    "steps = len(train_descriptions)\n",
    "# creating a directory named models to save our models\n",
    "os.mkdir(\"models\")\n",
    "for i in range(epochs):\n",
    "   generator = data_generator(train_descriptions, train_features, tokenizer, max_length)\n",
    "   model.fit_generator(generator, epochs=1, steps_per_epoch= steps, verbose=1)\n",
    "   model.save(\"models/model_\" + str(i) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddd4c2-55f0-42a9-80cc-fa8175053b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import argparse\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument('-i', '--image', required=True, help=\"Image Path\")\n",
    "args = vars(ap.parse_args())\n",
    "img_path = args['image']\n",
    "def extract_features(filename, model):\n",
    "try:\n",
    "           image = Image.open(filename)\n",
    "except:\n",
    "           print(\"ERROR: Can't open image! Ensure that image path and extension is correct\")\n",
    "       image = image.resize((299,299))\n",
    "       image = np.array(image)\n",
    "      # for 4 channels images, we need to convert them into 3 channels\n",
    "if image.shape[2] == 4:\n",
    "           image = image[..., :3]\n",
    "       image = np.expand_dims(image, axis=0)\n",
    "       image = image/127.5\n",
    "       image = image - 1.0\n",
    "       feature = model.predict(image)\n",
    "return feature\n",
    "def word_for_id(integer, tokenizer):\n",
    "for word, index in tokenizer.word_index.items():\n",
    "if index == integer:\n",
    "return word\n",
    "return None\n",
    "def generate_desc(model, tokenizer, photo, max_length):\n",
    "   in_text = 'start'\n",
    "for i in range(max_length):\n",
    "       sequence = tokenizer.texts_to_sequences([in_text])[0]\n",
    "       sequence = pad_sequences([sequence], maxlen=max_length)\n",
    "       pred = model.predict([photo,sequence], verbose=0)\n",
    "       pred = np.argmax(pred)\n",
    "       word = word_for_id(pred, tokenizer)\n",
    "if word is None:\n",
    "           break\n",
    "       in_text += ' ' + word\n",
    "if word == 'end':\n",
    "           break\n",
    "return in_text\n",
    "max_length = 32\n",
    "tokenizer = load(open(\"tokenizer.p\",\"rb\"))\n",
    "model = load_model('models/model_9.h5')\n",
    "xception_model = Xception(include_top=False, pooling=\"avg\")\n",
    "photo = extract_features(img_path, xception_model)\n",
    "img = Image.open(img_path)\n",
    "description = generate_desc(model, tokenizer, photo, max_length)\n",
    "print(\"nn\")\n",
    "print(description)\n",
    "plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
